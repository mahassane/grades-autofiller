{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import joblib\n",
    "import albumentations as A\n",
    "import imutils\n",
    "\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "path_to_dataset = './dataset'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_pixels(img):\n",
    "    return img.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    \n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "\n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HSV Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hsv_histogram(img, target_img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Extract an HSV histogram from an image.\n",
    "\n",
    "    Parameters:\n",
    "    - img: The input image in BGR format (as loaded by OpenCV).\n",
    "    - target_img_size (tuple): The desired size to resize the image (width, height).\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The normalized and flattened HSV histogram.\n",
    "    \"\"\"\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    hist = cv2.calcHist([hsv_img], channels=[0, 1, 2], mask=None, histSize=(8, 8, 8), ranges=[0, 180, 0, 256, 0, 256])\n",
    "\n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist)\n",
    "    else:\n",
    "        cv2.normalize(hist, hist)\n",
    "\n",
    "    return hist.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = A.Compose([\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.VerticalFlip(p=0.5),\n",
    "#     A.RandomRotate90(p=0.5),\n",
    "#     A.OneOf([\n",
    "#         A.GaussianBlur(p=0.2),\n",
    "#         A.MotionBlur(p=0.2),\n",
    "#         A.MedianBlur(blur_limit=3, p=0.2),\n",
    "#     ], p=0.5),\n",
    "#     A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.5),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import albumentations as A\n",
    "\n",
    "# # Define augmentation pipeline\n",
    "# augmentation_pipeline = A.Compose([\n",
    "#     A.HorizontalFlip(p=0.2),\n",
    "#     A.VerticalFlip(p=0.2),\n",
    "#     A.RandomRotate90(p=0.2),\n",
    "#     A.OneOf([\n",
    "#         A.GaussianBlur(p=0.5),\n",
    "#         A.MotionBlur(p=0.5),\n",
    "#         A.MedianBlur(blur_limit=3, p=0.2),\n",
    "#     ], p=0.5),\n",
    "#     A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.5),\n",
    "# ])\n",
    "\n",
    "# # Augment images and save in the same folder\n",
    "# def augment_and_save_inplace(input_dir):\n",
    "#     for dirpath, _, filenames in os.walk(input_dir):\n",
    "#         for fn in filenames:\n",
    "#             if not fn.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "#                 continue\n",
    "\n",
    "#             # Read the image\n",
    "#             img_path = os.path.join(dirpath, fn)\n",
    "#             img = cv2.imread(img_path)\n",
    "#             if img is None:\n",
    "#                 print(f\"Warning: Could not read image {img_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Perform augmentation\n",
    "#             augmented = transform(image=img)['image']\n",
    "\n",
    "#             # Generate new filename for the augmented image\n",
    "#             name, ext = os.path.splitext(fn)\n",
    "#             augmented_filename = f\"{name}_aug{ext}\"\n",
    "#             augmented_path = os.path.join(dirpath, augmented_filename)\n",
    "\n",
    "#             # Save the augmented image\n",
    "#             cv2.imwrite(augmented_path, augmented)\n",
    "\n",
    "# # Path to the dataset\n",
    "# path_to_dataset = \"./archive\"\n",
    "\n",
    "# # Augment and save\n",
    "# augment_and_save_inplace(path_to_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    features = {\n",
    "        \"raw\": [],\n",
    "        \"hog\": [],\n",
    "        \"hsv\": []\n",
    "    }\n",
    "    labels = []\n",
    "    labels_set = set()\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(path_to_dataset):\n",
    "        for fn in filenames:\n",
    "            if not fn.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "\n",
    "            label = os.path.basename(dirpath)  # Use the folder name as the label\n",
    "            labels.append(label)\n",
    "            labels_set.add(label)\n",
    "\n",
    "            path = os.path.join(dirpath, fn)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {path}\")\n",
    "                continue\n",
    "\n",
    "            # img = cv2.resize(img, target_img_size)  # Resize to fixed size\n",
    "            features[\"raw\"].append(extract_raw_pixels(img))\n",
    "            features[\"hog\"].append(extract_hog_features(img))\n",
    "            features[\"hsv\"].append(extract_hsv_histogram(img))\n",
    "        \n",
    "    if not features:\n",
    "        raise ValueError(\"No valid images were loaded. Please check the dataset path or image formats.\")\n",
    "    print(labels_set)\n",
    "    print(len(labels_set))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'SVM': svm.LinearSVC(random_state=random_seed),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7, n_jobs=-1),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "def classify_image(img, model_name, feature):\n",
    "\n",
    "    # Load the trained model\n",
    "    model = joblib.load(f\"./Models/{model_name}_{feature}_model.pkl\")\n",
    "    \n",
    "    # Convert image to grayscale if needed and resize to match training input size\n",
    "    # img = cv2.resize(img, target_img_size)  # Uncomment if resizing was used during training\n",
    "\n",
    "    # Extract raw pixel features and reshape to match the training input format\n",
    "    if feature == \"raw\":\n",
    "        img_features = extract_raw_pixels(img).reshape(1, -1)\n",
    "    elif feature == \"hog\":\n",
    "        img_features = extract_hog_features(img).reshape(1, -1)\n",
    "    elif feature == \"hsv\":\n",
    "        img_features = extract_hsv_histogram(img).reshape(1, -1)\n",
    "\n",
    "    # Predict the label\n",
    "    label = model.predict(img_features)\n",
    "\n",
    "    # print(f\"Predicted label: {label[0]}\")\n",
    "    return label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels):\n",
    "\n",
    "    train_features = {\n",
    "        \"raw\": None,\n",
    "        \"hog\": None,\n",
    "        \"hsv\": None\n",
    "    }\n",
    "\n",
    "    test_features = {\n",
    "        \"raw\": None,\n",
    "        \"hog\": None,\n",
    "        \"hsv\": None\n",
    "    }\n",
    "\n",
    "    train_labels = {\n",
    "        \"raw\": None,\n",
    "        \"hog\": None,\n",
    "        \"hsv\": None\n",
    "    }\n",
    "\n",
    "    test_labels = {\n",
    "        \"raw\": None,\n",
    "        \"hog\": None,\n",
    "        \"hsv\": None\n",
    "    }\n",
    "    \n",
    "    train_features[\"raw\"], test_features[\"raw\"], train_labels[\"raw\"], test_labels[\"raw\"] = train_test_split(\n",
    "        features[\"raw\"], labels, test_size=0.3, random_state=random_seed)\n",
    "    \n",
    "    train_features[\"hog\"], test_features[\"hog\"], train_labels[\"hog\"], test_labels[\"hog\"] = train_test_split(\n",
    "        features[\"hog\"], labels, test_size=0.3, random_state=random_seed)\n",
    "    \n",
    "    train_features[\"hsv\"], test_features[\"hsv\"], train_labels[\"hsv\"], test_labels[\"hsv\"] = train_test_split(\n",
    "        features[\"hsv\"], labels, test_size=0.3, random_state=random_seed)\n",
    "    \n",
    "    for model_name, model in classifiers.items():\n",
    "        print('############## Training', model_name, \"##############\")\n",
    "        for feature, label in zip(features.keys(), train_labels.keys()):\n",
    "            model.fit(train_features[feature], train_labels[label])\n",
    "\n",
    "            accuracy = model.score(test_features[feature], test_labels[label])\n",
    "\n",
    "            print(model_name, 'with', f\"{feature} features\", 'accuracy:', accuracy*100, '%')\n",
    "            # Save the trained model\n",
    "            joblib.dump(model, f\"./Models/{model_name}_{feature}_model.pkl\")\n",
    "\n",
    "            print(f\"{model_name}_{feature} model saved successfully.\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "{'L', 'P', 'V', 'R', 'E', 'H', 'O', 'Q', 'W', 'Z', 'A', 'X', 'C', 'F', 'D', 'B', 'N', 'J', 'I', 'S', 'U', 'T', 'M', 'Y', 'K', 'G'}\n",
      "26\n",
      "Finished loading dataset.\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset. This will take time ...')\n",
    "features, labels = load_dataset()\n",
    "print('Finished loading dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Training SVM ##############\n",
      "SVM with raw features accuracy: 71.90243902439025 %\n",
      "SVM_raw model saved successfully.\n",
      "SVM with hog features accuracy: 91.70731707317074 %\n",
      "SVM_hog model saved successfully.\n",
      "SVM with hsv features accuracy: 10.975609756097562 %\n",
      "SVM_hsv model saved successfully.\n",
      "\n",
      "############## Training KNN ##############\n",
      "KNN with raw features accuracy: 81.17073170731707 %\n",
      "KNN_raw model saved successfully.\n",
      "KNN with hog features accuracy: 93.46341463414635 %\n",
      "KNN_hog model saved successfully.\n",
      "KNN with hsv features accuracy: 12.341463414634147 %\n",
      "KNN_hsv model saved successfully.\n",
      "\n",
      "############## Training RandomForest ##############\n",
      "RandomForest with raw features accuracy: 83.7560975609756 %\n",
      "RandomForest_raw model saved successfully.\n",
      "RandomForest with hog features accuracy: 90.09756097560975 %\n",
      "RandomForest_hog model saved successfully.\n",
      "RandomForest with hsv features accuracy: 15.121951219512194 %\n",
      "RandomForest_hsv model saved successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage import io\n",
    "from skimage.util import invert\n",
    "\n",
    "img = io.imread('test.jpg')\n",
    "inverted_img = invert(img)\n",
    "\n",
    "classify_image(inverted_img, \"RandomForest\", \"hog\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
